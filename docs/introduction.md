## 课程信息

本门课程涵盖的主题：
  1. 问题定义（Problem definitions）
  2. 多任务学习基础（Multi-task learning basics）
  3. 元学习算法：黑箱方法，基于优化的元学习，度量学习（Meta-learning algorithms: black-box approaches, optimization-based meta-learning, metric learning）
  4. 分层贝叶斯模型 & 元学习（Hierarchical Bayesian models & meta-learning）
  5. 多任务强化学习，目标控制的强化学习，分层强化学习（Multi-task RL, goal-conditioned RL, hierarchical RL）
  6. 元强化学习（Meta-reinforcement learning）
  7. 开放问题，受邀演讲，研究讲座（Open problems, invited lectures, research talks）

这门课程会着重于深度学习和强化学习，不会包含架构搜索、超参数优化、学习优化器在内的 AutoML 主题。

## 背景

- **如何得到一个更加通用的 AI 系统？**

机器人能够教我们什么是智能，它有以下特点：

* 与真实世界交互
* 必须能够跨任务、对象、环境实现泛化
* 需要一些常识性知识才能把事情做好
* 监督不是理所当然的，提供标签以及指出标签如何指导机器人学习都很难

一个问题是，在真实机器人上进行实验时，为每个任务都收集大量数据是费时费力、不切实际的。但是，机器人需要依赖**详细的监督信息和指导**，才能在**一个环境**中**从头开始**学习**一个任务**。它能够针对一个任务成为专家，但是在新任务上或者到了新环境就一无所知。并且这不只是在强化学习和机器人学上的问题，在机器翻译、语音识别、目标检测等上同样如此。每次到要从头学习新任务是无法实现通用 AI 系统的。然而，人类却是天生的泛化专家，即使是婴儿都可以通过在丰富多样的环境中进行交互来学习许多简单的技能，并利用学习经验尝试解决更复杂的任务。

- **当没有大型数据集时怎么办？**

深度学习使得我们不需要手工进行特征工程和过多的特定领域知识，就能够处理非结构化的数据。同时，一个趋势是大规模多样化的数据集结合超大规模模型，以通过深度学习实现广阔的泛化，这样的例子包括 ImageNet, Transformer, GPT-2 等等。然而，不是对所有任务都有大型数据集，因此我们需要让多任务学习和元学习与深度学习相结合。

- **当数据中有长尾时怎么办？**

![](https://raw.githubusercontent.com/bighuang624/pic-repo/master/CS330-long-tail.png)

“长尾”指正态曲线两边相对平缓的部分。分布在尾部的数据是零散的、小量的样例，但是总体数量会比分布在头部的样例数量更大。这种设置打破了标准的机器学习范式，有监督的学习方法很难在这些数据上表现良好。

- **当需要快速学习新事物时怎么办？**

我们非常希望机器不必从头开始学习就能够实现对新知识（包括新的人物，新的任务，新的环境，等等）的快速学习，即只使用极少量的数据就能够较好地完成新任务。这个问题被称为**小样本学习（Few-Shot Learning）**。实现的方式是利用先验经验，例如给我们两位风格截然不同的画家的几幅作品，让我们判断下一幅图是谁的作品。我们可能之前没有看过这两位画家的画作，但是以前观看其他人的画作所学到的知识能够帮助我们来快速分辨出作者。

---

**以上问题是多任务学习和元学习能够发挥作用的地方。**

## 问题定义

什么是任务？在这里，我们可以将任务定义为给定数据集和损失函数，学习得到模型。因此，不同任务的区别可能在于不同的对象、不同的人、不同的目标、不同的光照条件、不同的词语、不同的语言等等，而不仅限于字面意思上的“任务”（例如图像分类和语义分割）。即，任务的不同可能在于数据集或/与损失函数的不同。

多任务学习需要遵循的一个关键假设是，不同任务需要共享一些结构。好在，能够共享结构的任务有很多：

![](https://raw.githubusercontent.com/bighuang624/pic-repo/master/CS330-some-similar-movements.png)

即使很多任务看上去不相关，也有很多法则（或者说，先验知识）能够帮助各种多任务学习的实现，例如：

* 真实数据都遵循物理定律；
* 人都是有感染力的生物；
* 英语语言数据都遵循英语规则；
* 语言的发展都出自相似的目的。

**非正式的问题定义**

* **多任务学习问题**：与彼此独立地学习各个任务相比，更快或更熟练地学习所有任务。
* **元学习问题**：根据先前一系列任务的数据/经验，可以更快和/或更有效地学习新任务。

跨任务聚合数据并学习单个模型是多任务学习的一种方法。但是，通过利用我们知道数据来自不同任务这一事实，通常能够达到更好的多任务学习效果。

**为什么要学习多任务学习和元学习？**

1. 多任务学习和元学习正在成为机器学习研究中一个备受关注的问题；
2. 经典的多任务学习和元学习算法仍在持续作为最新研究的基石；
3. 多任务学习和元学习的成功对于深度学习的“平民化”至关重要。

## 参考资料

* 本节内容对应的 [PPT](http://web.stanford.edu/class/cs330/slides/cs330_lecture1.pdf)